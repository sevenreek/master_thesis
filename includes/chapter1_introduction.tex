\section{Introduction}


\subsection{Motivation}
  Concerns regarding the sustainability of using fossil fuels for energy generation
  have been raised in as early as the 1970s \cite{rethinking_resource_depletion}. 
  A 1972 report written in 1972 titled "Limits to Growth" by Meadows et. al.
  \cite{limits_to_growth} is often brought up as an example.
  In it a group of MIT scientists attempted to answer the question of
  how long will the Earth's natural resources last for,
  considering the seemingly neverending growth of human civilisation.
  As a result of a computer simulation,
  a rough estimate of around 100 years was given as the deadline.
  After that time the population would start to collapse due to the lack of resources.


  This estimate did not go without controversies back when it was first published.
  The methodology was thoroughly picked apart,
  leading many to dismiss the study findings
  \cite{rethinking_resource_depletion}.
  Naturally, nowadays, we are much better suited to verify
  the claims made by the now 50 year old book. The impeding resource depletion
  has been made a less valid claim as technological progress
  made it possible to locate and tap into
  previously inaccessible resource fields
  \cite{shaping_the_global_oil_peak}.
  Taking into account other issues, however, 
  the original timeline of 100 years might have
  actually shifted closer. 


  When it comes to fossil fuel usage, in the last twenty years, 
  the primary concerns have changed from resource depletion to global warming 
  and irreversible environmental damage \cite{rethinking_resource_depletion}.
  In 2018 the Intergovermental Panel on Climate Change (IPCC) published a
  report indicating the need to stop the average global temperature 
  at 1.5\degree C above the levels measurable in the pre-industrial era.
  The panel projected that failure to do so, would lead to irreversible climate changes
  and in turn serious damage to human settlements around the world.
  \cite{ipcc2018}


  Fossil fuels account for as much as 70\% of greenhouse gas emissions.
  Electricity generation alone causes 25-35\% 
  \cite{global_climate_change}.
  Such a high share means that reducing this output
  is critical to accomplish the goals outlined by the IPCC.
  At the beginning of the twentieth century, renewable energies, i.e. 
  wind, solar, biomass and geothermal were thought
  to be the perfect solution to the issue at hand
  \cite{renewable_review_2000}. 


  In modern times, we have now become aware of multiple issues
  that make renewable energy generation a problem at a larger scale.
  Most importantly, their efficacy varies depending on the geographical
  location and climate. Even when placed in optimal conditions,
  they do not offer perfect stability. The energy output varies
  depending on the weather and time of day. Additionally, the land
  usage is greater than with the more traditional forms of energy production
  \cite{renewable_problems}.

\subsection{Fission energy}

  In recent years, the slow implementation of renewable energies has 
  led to the formation of an alternative approach in both research and policymaking. 
  The use of nuclear energy for supplementing the shortcomings 
  of renewables has been suggested as a potential path forward.
  This concept is sometimes referred to as a hybrid nuclear-renewable system.
  \cite{hybrid_nuclear_renewable}. 

  There are two ways that nuclear energy can be created and harnessed.
  In the more well-established technology, fission, heavy atoms 
  (usually Uranium) are bombarded with neutrons 
  and split into two or more lighter nuclei and
  additional neutrons as shown in \autoref{fig:fission}.
  The reaction is self-sustaining 
  and releases energy in the form of heat, that is then used
  to boil water. The produced steam then causes turbines to spin
  and generate electricity.
	\begin{figure}[H]
	  \centering
	  \includegraphics[width=.75\linewidth]{media/fission.png}
	  \caption{Uranium fission reaction}
	  \label{fig:fission}
	\end{figure}

  Fission is far from a new concept, as first fission reactors have been 
  built in as early as 1942 \cite{first_fission_reactor}. 
  Although the technology itself is quite old 
  and has been greatly improved over time, 
  there is reasonable reluctance to build and use fission power plants. 
  The issue that gets raised most often is the storage of radioactive waste. 
  There are, however, a few other problems with fission \cite{fission_problems}. 


  The tragedies of Chornobyl and Fukushima reactors
  have caused many people to be wary of fission. Even if
  societal support would be disregarded in policymaking, the acquisition,
  storage and disposal of radioactive materials required for and produced 
  during fission proves to be an administrative challenge.
  This is particularly problematic 
  if reactor construction and maintenance is to be handled
  by private entities \cite{fission_tech_and_current_issues}. 
  The complexity of the problem with energy production suggests that 
  as we arrive to more concrete solutions we 
  should not stop exploring other potential alternatives.

\subsection{Fusion energy}

  Just like it is possible to split atoms, it is also possible to
  combine them together in a process known as fusion. 
  Fusing atoms lighter than Iron is a reaction that 
  can produce surplus energy as highlighted by \autoref{fig:binding_energy}.
  Two atoms of low binding energy can fuse into an atom of higher 
  binding energy, releasing heat based on the mass-energy equivalence formula.
  The output energy can be used to generate electricity in
  the exact same manner as with fission. 
  The problem with fusion reactors is that the conditions necessary
  for fusion to happen are extremely harder to achieve and sustain 
  \cite{structural_materials_fusion}.

  \begin{figure}[H]
	\centering
	\includegraphics[width=.90\linewidth]{media/binding_energy.png}
	\caption{Average binding energy per nucleon as a function of the number of nucleons in the atom}
	\label{fig:binding_energy}
  \end{figure}

  Fusion is the primary reaction that causes stars to emit light and heat.
  The reaction that is most often artificially attempted on Earth differs
  from that occurring naturally in the Sun. There, a p-p reaction occurs,
  4 protons turn into ${}^{4}$He. The p-p reaction is not the most efficient but
  the Sun's gravitational field makes it feasible.
  On Earth, fusion experiments primarily rely on using hydrogen isotopes, 
  most commonly deuterium (D) and tritium (T) as shown in \autoref{fig:fusion}.
  This is a considerably simpler reaction that offers the most promise
  when it comes to fusion devices.

  \begin{figure}[H]
	\centering
	\includegraphics[width=.75\linewidth]{media/fusion.png}
	\caption{D-T fusion reaction}
	\label{fig:fusion}
  \end{figure}

  Despite being an easier approach, it still requires the reactor to sustain
  a 200 million \degree C plasma. This means that a large amount of energy
  must be used to first heat the plasma up and then confine it to 
  prevent it from completely destroying the reactor. 
  The efficiency of D-T reactions might, however, be worth the trouble.
  Theoretically, just 30 mg of deuterium would generate as much energy
  as 250 l of gasoline \cite{nuclear_fusion_status}. 


  To achieve this output a few hurdles must first be overcome.
  Tritium, the other input material of the D-T reaction is
  extremely rare in nature. Its artificial production is currently 
  done only by a select number of facilities. 
  Combined with its relatively short half-life of around 12 years, 
  concerns regarding tritium shortage have recently been raised.
  Future fusion reactors like, should be able to "breed" their own tritium,
  however the transition period 
  may still prove to be troublesome \cite{fusion_fuel_running_out}.


  In the end, despite being a similarly old technology as fission
  \cite{fusion_history},
  a fusion reactor with a net positive energy balance
  has not yet been constructed. Containing plasma heated to such extreme
  temperatures cannot be achieved with any solid material and must 
  be done with the use of inertial or magnetic forces. 
  The most common reactors that rely on magnetic confinement are:
  tokamaks (\autoref{fig:iter_reactor}) 
  and stellarators (\autoref{fig:w7x_reactor}).
  \cite{tokamak_vs_stellarator}

  \begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
      \centering
      \includegraphics[width=\linewidth]{media/iter_reactor_3d.jpeg}
      \caption{ITER tokamak model \cite{iter_website}}
      \label{fig:iter_reactor}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
      \centering
	  \includegraphics[width=\linewidth]{media/w7x_3d.png}
      \caption{Wendelstein W7X sterellator model \cite{w7x_website}}
      \label{fig:w7x_reactor}
    \end{minipage}
  \end{figure}

  Stellarators and tokamaks share some common features,
  and have been discovered within a few years of each other.
  In the 20th century the stellarator design proved 
  to be more complex to construct than the tokamak design.
  For that reason significantly more research was done with tokamaks.
  Although interest in stellarators has resurfaced recently with
  the Wendelstein W7X project,
  the International Thermonuclear Experimental Reactor (ITER)
  is going to be a tokamak.
  \cite{nuclear_fusion_status}

\subsection{ITER Tokamak Project}
  \subsubsection{Tokamak}
	\begin{figure}[H]
	  \centering
	  \includegraphics[width=.70\linewidth]{media/iter_crosssection.jpeg}
	  \caption{ITER tokamak cross section \cite{iter_website}}
	  \label{fig:iter_crosssection}
	\end{figure}
	Tokamaks are reactors composed of a toroidal vacuum chamber surrounded by
	massive electromagnetic coils as shown in \autoref{fig:iter_crosssection}.
	The magnetic fields are used to shape plasma 
	and keep it away from the device internal walls. 
	Operation starts with the removal of air and impurities from the vessel.
	The gaseous fuel is introduced and a strong current is induced 
	ionizing the gases, thus forming plasma. Additional heat is introduced with
	microwaves and fuel injections. After a specific temperature is reached, 
	particles within the plasma can collide with such force that they begin
	to break through atomic repulsion and fuse producing a large amount of energy.
	\cite{iter_website}
  \subsubsection{ITER Goals}

	At 24 m height and 30 m width, 
	ITER will be the biggest tokamak ever constructed. 
	That's twice as big as the largest experimental reactor currently in service, 
	the Joint European Torus (JET). JET has been working since 1983 with 
	a goal of achieving a net energy gain. Despite running successful 
	plasma experiments and recently starting deuterium-tritium experiments,
	the highest Q 
	(the ratio of produced power to the power required to sustain the plasma)
	obtained by JET was 0.33. A record only recently topped by a different 
	type of a fusion device at the US Department of Energy’s
	National Ignition Facility. Using laser technology it was possible to
	obtain a Q of 0.7 for 4 billionths of a second. \cite{fusion_records}
	

	Although not supposed to work as a power plant, 
	throughout its operation ITER intends to break the record by a lot.
	The new reactor is designed to reach an energy gain of 10 
	for the duration of a few minutes. This is still a theoretical
	plan, as the facility is still a long time from being finished.
	As of 2022, the vacuum vessel must still be welded together\cite{iter_timeline}.
	First plasma is scheduled for the end of 2025\cite{iter_timeline},
	although multiple delays have already happened in the past \cite{iter_delays},
	so future ones would, at this point, not be too surprising. 
	\autoref{fig:iter_aerial} shows an aerial picture of ITER from 14th of April 2020.
	\begin{figure}[H]
	  \centering
	  \includegraphics[width=.8\linewidth]{media/iter_aerial_2022.jpg}
	  \caption{Aerial view of the ITER facility as of 2022 \cite{iter_website}}
	  \label{fig:iter_aerial}
	\end{figure}

\subsection{Real time diagnostics and plasma control}

	With a project of such large magnitude as ITER it is near impossible to predict
	everything that might go wrong. There are naturally safety concerns,
	questions regarding the ambitious goals set by the management and
	the troubles of international cooperation\cite{iter_delays}.
	It is thus crucial to maintain maximum operational safety 
	and log all experimental data. This should ensure, that even if all
	of the project goals are not fulfilled, important practical knowledge
	can be recorded for future fusion experiments.


	When operational, ITER will rely on around 50 completely different
	measurement systems to control, evaluate and investigate its plasma 
	\cite{iter_diagnostics_count}.
	This translates into dozens of gigabytes of data being generated, 
	processed and archived every second as the experiment is running 
	\cite{iter_data_throughput}.
	With some of the experiments lasting fractions of a second,
	a lot of the critical data acquisition and control must occur
	in real time and without waiting for human input 
	\cite{iter_realtime_processing}.
	This poses an important challenge when it comes to the choice
	of computing apparatus. A perfect device would offer
	infinite configurability preferably with remote access 
	as well as a very high data throughput. 


	Traditional computers or more precisely Central Processing Units (CPUs) 
	are reprogrammable but may offer insufficient speeds in 
	some of the real time applications. On the other end of this scale lie 
	Application Specific Integrated Circuits (ASICs). These devices are an 
	arrangement of digital logic gates realizing one specific goal, like 
	digital signal filtering or processing network packets.
	ASICs offer unmatched speeds but cannot really be reconfigured
	after deployment, making them a risky choice in highly experimental
	applications such as tokamaks.

  \subsubsection{Field Programmable Gate Arrays}
    Devices that offer a compromise between speed and reconfigurability
    exist nowadays. Most commonly Field Programmable Gate Arrays (FPGAs) are used.
    FPGAs are formed out of matrices of Configurable Logic Blocks (CLBs).
    CLBs are small circuits that produce a single bit of logic output
    out of 4 input bits, using a reprogrammable function. These blocks 
    are then wired together using programmable interconnects.
	A finite combination of CLBs can realise any digital function.
	Such design allows for the implementation of very efficient algorithms
    directly with the use of logical gates, without the need for an
    entire processor. \cite{xilinx_what_is_fpga}


    The interconnects and CLB internal structure introduce additional wiring
    that would be unnecessary in the case of ASICs. These stray capacitances 
    and inductances limit the maximum clock speed of FPGAs. This not too much
    of an issue provided that the function can be parallelised and pipelined.
	In addition, to offset this limitation, most FPGAs come packed with 
    more complex sub-circuitry that can be intermixed with the CLBs
    as indicated in \autoref{fig:fpga_structure}.
    These components can vary greatly from efficient arithmetical blocks
    to small CPU-based microcontrollers.

    \begin{figure}[H]
      \centering
      \includegraphics[width=.6\linewidth]{media/fpga_structure.png}
      \caption{The internal structure of an FPGA}
      \label{fig:fpga_structure}
    \end{figure}
    

\subsection{Problem statement}

The nature of FPGAs makes them instrumental in data acquisition 
and control systems of modern fusion experiments like the ITER tokamak.
This work evaluates the usage of Field Programmable Gate Arrays
for the detection and analysis of pulses produced by PhotoMultiplier Tubes (PMTs)
being part of a High X-Ray Monitor (HXRM) designed to monitor Runaway Electorns
(REs) in tokamaks.


A functional system for the Digital Pulse Processing (DPP) at the rate of 
1 GS/s is proposed and implemented in an FPGA. A custom
software package for control and data acquisition is shown.
The optimization of both hardware and software is described.
Different algorithms for pulse detection and discrimination
are analysed, simulated and implemented in hardware. Experimental
results produced by the system are described. Finally design
considerations for future systems are given.

